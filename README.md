# Gesture Control â€” Jetson Nano HCI Prototype

Real-time hand gesture recognition for media control, powered by **MediaPipe** and **OpenCV**.  
A minimalist web dashboard streams the camera feed and displays live gesture metrics.

---

## Gestures

| Gesture | Action | Key |
|---------|--------|-----|
| ðŸ– Open Palm | Play / Pause | `playpause` |
| â˜ï¸ Index Up | Volume Up | `volumeup` |
| âœŠ Fist | Mute | `volumemute` |

## Quick Start

> **Requires Python 3.10 â€“ 3.12** (MediaPipe is not yet compatible with 3.13 on Windows).

```bash
# 1. Clone and enter the repo
git clone https://github.com/Udit-H/Gesture-Jetson_Nano.git
cd Gesture-Jetson_Nano

# 2. Create a venv with a compatible Python version
py -3.12 -m venv venv        # Windows
python3.12 -m venv venv      # macOS / Linux

# 3. Activate and install
.\venv\Scripts\activate       # Windows
source venv/bin/activate      # macOS / Linux
pip install -r requirements.txt

# 4. Run the dashboard
python app.py
```

Open **http://localhost:5000** in your browser.

## Project Structure

```
â”œâ”€â”€ app.py                  # Flask server + gesture engine
â”œâ”€â”€ cursor.py               # Standalone cursor-control script
â”œâ”€â”€ media_control.py        # Standalone media-control script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css           # Minimalist black & white theme
â””â”€â”€ templates/
    â””â”€â”€ index.html          # Dashboard UI
```

## Tech Stack

- **MediaPipe** â€” Hand landmark detection (21-point model)
- **OpenCV** â€” Camera capture & frame processing
- **Flask** â€” Lightweight MJPEG streaming server
- **PyAutoGUI** â€” System-level keyboard simulation

## Notes

- The `hand_landmarker.task` model is downloaded automatically on first run.
- For Jetson Nano deployment, enable the GPU delegate in `app.py` for better performance.
